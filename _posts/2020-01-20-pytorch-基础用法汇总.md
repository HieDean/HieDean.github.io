---
layout:     post
title:      "pytorch 基础用法汇总"
subtitle:   ""
date:       2020-1-20
author:     "HieDean"
header-img: "img/4.jpg"
tags:
    - 天大毕设
    - pytorch
---
### tensor的合并
cat会在原有维度上合并两个tensor

``torch.cat([tensor1, tensor2], dim=0)``

stack会增添一个新的维度

``torch.stack([tensor1, tensor2], dim=0)``

### tensor的点乘
直接用``*``就可以了

### GPU是否可用
`torch.cuda.is_available()`

### 有几块GPU
`torch.cuda.device_count()`

### 第0块GPU的名字
`torch.cuda.get_device_name(0)`

### 使用指定GPU
`import os`
`os.environ["CUDA_VISIBLE_DEVICES"] = "0"`

### 使用GPU加速
```python
# 将模型与损失函数转移至GPU
model = Model()
loss_fun = torch.nn.CrossEntropyLoss()
model = model.cuda()
loss_f = loss_fun.cuda()
# 输入的数据转移至GPU
x,y = x.cuda(),y.cuda()
# 最后结果转移回CPU
loss = loss.cpu()
# 注意优化器不需要进行这一步
```

### 打印模型的参数名与参数值
```python
# 打印参数名
for x in model.state_dict():
    print(x)
# 打印参数名与参数值
for x in model.named_parameters():
    print(x)
```

### 为什么训练模型时显存占用没问题，但在测试模型时OOM？
在测试模型的时候一定要加上
```python
with torch.no_grad(): 
# test_code
```
否则被测试的模型所占用的显存会越来越大最后出现OOM

### torch.manual_seed()
设置GPU初始化时种子

### nn.DataParallel(model, device_ids=device_ids)
多GPU并行运算使用

### torch.nn.utils.clip_grad_norm(parameters, max_norm, norm_type=2)
用于限制梯度，防止梯度爆炸或消失

### pytorch保存模型并继续训练
参考文章 https://www.jianshu.com/p/1cd6333128a1

